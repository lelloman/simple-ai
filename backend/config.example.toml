# SimpleAI Backend Configuration
# Copy to config.toml and adjust values.
# Environment variables override these settings (SIMPLEAI__SECTION__KEY format).

# Server settings
host = "0.0.0.0"
port = 8080

[ollama]
# Direct Ollama URL (used when gateway mode is disabled)
base_url = "http://localhost:11434"
# Default model for users with model:specific role
model = "llama3.2"

[oidc]
# Required: Your OIDC provider's issuer URL
issuer = "https://auth.example.com/realms/main"
# Required: OIDC client ID (audience claim in tokens)
audience = "simple-ai"

[database]
url = "sqlite:./data/audit.db"

[logging]
level = "info"

[cors]
origins = "*"

[gateway]
# Enable gateway mode to route requests to connected inference runners
enabled = false
# Authentication token for runner WebSocket connections
auth_token = "change-me-in-production"
# Timeout for stale runner removal (seconds)
runner_timeout_secs = 60
# Enable wake-on-demand when no runners available
auto_wake_enabled = false
# Timeout waiting for runner to wake (seconds)
wake_timeout_secs = 90
# URL of idle-manager service (optional, for orchestrated wake)
# idle_manager_url = "http://idle-manager:8090"

[wol]
# Broadcast address for WOL packets
broadcast_address = "255.255.255.255"
# UDP port for WOL packets
port = 9
# URL of WOL bouncer service (for Docker environments)
# bouncer_url = "tcp://host.docker.internal:9999"

[models]
# Model classification for routing and permissions.
# Users with "model:specific" role can request exact models.
# Users without it can only request classes: "class:fast" or "class:big".
# Models not in either list are not available for class-based requests.
big = [
    "llama3:70b",
    "qwen2:72b",
    "deepseek:67b",
]
fast = [
    "llama3:8b",
    "mistral:7b",
    "phi3:mini",
]

[routing]
# Smart routing configuration for intelligent request distribution.
#
# queue_weight: Balance between preference (0) and queue depth (1).
# Higher values prefer runners with fewer pending requests.
queue_weight = 0.5
#
# latency_weight: Weight for historical latency metrics (0 to 1).
# Higher values prefer runners with lower average latency.
latency_weight = 0.3
#
# speculative_wake_enabled: Wake multiple machines in parallel for faster response.
# When enabled, the system wakes all configured targets and uses the first to connect.
speculative_wake_enabled = false

# Machine type preferences by model class.
# First machine type in the list is most preferred.
# Runners with machine types not in the list are deprioritized.
[routing.class_preferences]
fast = ["gpu-server", "halo"]   # Prefer gpu-server for fast models
big = ["halo", "gpu-server"]    # Prefer halo for big models

# Speculative wake targets by model class.
# When speculative_wake_enabled=true and no runners are available,
# wake ALL machines in this list for the requested class.
[routing.speculative_wake_targets]
fast = ["halo", "gpu-server"]   # Wake both; use whichever boots first
big = ["halo"]                  # Only halos can run big models
