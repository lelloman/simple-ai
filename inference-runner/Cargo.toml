[package]
name = "inference-runner"
description = "SimpleAI Inference Runner - abstracts local inference engines, exposes OpenAI-compatible API"
version.workspace = true
edition.workspace = true
authors.workspace = true

[[bin]]
name = "inference-runner"
path = "src/main.rs"

[dependencies]
# Internal crates
simple-ai-common = { workspace = true }

# Web framework
axum = { workspace = true }
tokio = { workspace = true }
tower-http = { workspace = true }

# Serialization
serde = { workspace = true }
serde_json = { workspace = true }

# HTTP client (for Ollama)
reqwest = { workspace = true }

# Async traits
async-trait = { workspace = true }

# Logging
tracing = { workspace = true }
tracing-subscriber = { workspace = true }

# Configuration
config = { workspace = true }

# Utilities
chrono = { workspace = true }
uuid = { workspace = true }
thiserror = { workspace = true }

# WebSocket (Phase 2)
tokio-tungstenite = { workspace = true }
futures-util = { workspace = true }

# Unix signal handling (Phase 3 - llama.cpp process management)
[target.'cfg(unix)'.dependencies]
nix = { version = "0.29", features = ["signal"] }

[dev-dependencies]
wiremock = "0.6"
tempfile = "3.8"
tower = "0.5"
