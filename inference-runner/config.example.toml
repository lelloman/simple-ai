# Example configuration for inference-runner

[runner]
id = "gpu-server-01"
name = "Primary GPU Server"
machine_type = "gpu-server"

[api]
host = "0.0.0.0"
port = 8080

# Gateway connection (optional, for fleet integration)
# When configured, the runner will maintain a WebSocket connection to the gateway
# and report its status (capabilities, model states, health) periodically.
#
# IMPORTANT: Use wss:// in production to encrypt the connection and protect the auth token.
# The auth_token should be a secure, randomly-generated secret shared with the gateway.
# Generate one with: openssl rand -hex 32
#
# [gateway]
# ws_url = "wss://gateway.example.com/ws/runners"  # Use wss:// in production
# auth_token = "your-secret-token"                 # Shared secret with gateway
# reconnect_delay_secs = 5                         # Time between reconnection attempts
# heartbeat_interval_secs = 30                     # How often to send status updates

[engines.ollama]
enabled = true
base_url = "http://localhost:11434"

# [engines.llama_cpp]
# enabled = false
# model_dir = "/models"
# server_binary = "/usr/bin/llama-server"

[capabilities.persistence]
always_loaded = []
idle_timeout_secs = 300
max_loaded_models = 3

# Map models to capabilities
# [capabilities.mappings]
# "llama3.2:3b" = ["fast_chat"]
# "qwen2.5:72b" = ["large_chat"]
# "nomic-embed-text" = ["embeddings"]
