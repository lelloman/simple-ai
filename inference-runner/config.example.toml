# Example configuration for inference-runner

[runner]
id = "gpu-server-01"
name = "Primary GPU Server"
machine_type = "gpu-server"

[api]
host = "0.0.0.0"
port = 8080

# Gateway connection (optional, for fleet integration)
# When configured, the runner will maintain a WebSocket connection to the gateway
# and report its status (capabilities, model states, health) periodically.
#
# IMPORTANT: Use wss:// in production to encrypt the connection and protect the auth token.
# The auth_token should be a secure, randomly-generated secret shared with the gateway.
# Generate one with: openssl rand -hex 32
#
# [gateway]
# ws_url = "wss://gateway.example.com/ws/runners"  # Use wss:// in production
# auth_token = "your-secret-token"                 # Shared secret with gateway
# reconnect_delay_secs = 5                         # Time between reconnection attempts
# heartbeat_interval_secs = 30                     # How often to send status updates

[engines.ollama]
enabled = true
base_url = "http://localhost:11434"

# llama.cpp engine configuration
# Manages llama-server subprocesses for inference. Each loaded model runs
# in its own server process for isolation.
#
# [engines.llama_cpp]
# enabled = false
# model_dir = "/models"                    # Directory containing .gguf model files
# server_binary = "/usr/bin/llama-server"  # Path to llama-server binary
# gpu_layers = 35                          # Layers to offload to GPU (-ngl flag)
# context_size = 4096                      # Context window size (-c flag)
# base_port = 9000                         # Optional: fixed base port (default: OS-assigned)
# max_servers = 2                          # Max concurrent model servers (default: 2)
# startup_timeout_secs = 120               # Server startup timeout (default: 120)
# shutdown_timeout_secs = 10               # Graceful shutdown timeout (default: 10)
# log_server_output = false                # Log llama-server output for debugging

[capabilities.persistence]
always_loaded = []
idle_timeout_secs = 300
max_loaded_models = 3

# Map models to capabilities
# [capabilities.mappings]
# "llama3.2:3b" = ["fast_chat"]
# "qwen2.5:72b" = ["large_chat"]
# "nomic-embed-text" = ["embeddings"]
